import re
import asyncio
import aiohttp
import pandas as pd
from io import BytesIO
from datetime import datetime
from tqdm.asyncio import tqdm
from models.trading_result import TradingResult
from database import AsyncSessionLocal
from constants import (
    EXCEL_ENGINE,
    COLUMN_PATTERNS,
    DATE_FORMAT,
    DATE_FORMAT_SPIMEX,
    BASE_URL,
    PAGE_URL_PATTERN,
    FILE_URL_PATTERN,
    ALL_FILES_PATTERN,
    RECORDS_TO_SAVE,
    METRIC_TON_MARKER,
    NUMERIC_COLUMNS,
    MAX_PAGES_TO_CHECK,
    HTTP_TIMEOUT,
)


PAGE_CACHE: dict = {}


def clear_page_cache():
    """–û—á–∏—â–∞–µ—Ç –∫—ç—à —Å—Ç—Ä–∞–Ω–∏—Ü"""
    global PAGE_CACHE
    PAGE_CACHE.clear()


def get_cache_stats():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞"""
    return {"cached_pages": len(PAGE_CACHE), "pages": list(PAGE_CACHE.keys())}


def extract_metric_ton_data(file_content, date_str):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å–µ–∫—Ü–∏–∏ '–ú–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è —Ç–æ–Ω–Ω–∞' Excel —Ñ–∞–π–ª–∞
    """
    try:
        df_raw = pd.read_excel(
            BytesIO(file_content),
            engine=EXCEL_ENGINE,
            header=None,
        )

        metric_ton_row = None
        for idx, row in df_raw.iterrows():
            row_str = " ".join([str(cell) for cell in row if pd.notna(cell)])
            if METRIC_TON_MARKER in row_str:
                metric_ton_row = idx
                break

        if metric_ton_row is None:
            return None

        header_row = metric_ton_row + 1
        df_section = pd.read_excel(
            BytesIO(file_content), engine=EXCEL_ENGINE, header=header_row
        )

        df_section.columns = [
            str(col).replace("\n", " ").strip() for col in df_section.columns
        ]

        column_mapping = find_columns(df_section.columns)

        if not column_mapping:
            return None

        df_filtered = df_section[list(column_mapping.values())].copy()

        code_col = column_mapping.get("exchange_product_id")
        if not code_col:
            return None

        df_filtered = df_filtered[df_filtered[code_col].notna()]
        df_filtered = df_filtered[df_filtered[code_col].astype(str).str.len() > 3]

        df_filtered = df_filtered.rename(
            columns={v: k for k, v in column_mapping.items()}
        )

        numeric_columns = NUMERIC_COLUMNS
        for col in numeric_columns:
            if col in df_filtered.columns:
                df_filtered[col] = pd.to_numeric(
                    df_filtered[col].replace("-", None), errors="coerce"
                )

        df_filtered = df_filtered.dropna(subset=NUMERIC_COLUMNS, how="all")

        df_filtered = df_filtered[df_filtered["exchange_product_id"].notna()]
        df_filtered = df_filtered[
            ~df_filtered["exchange_product_id"]
            .astype(str)
            .str.contains("–ò—Ç–æ–≥–æ", na=False)
        ]
        df_filtered = df_filtered[
            df_filtered["exchange_product_id"].astype(str) != "nan"
        ]

        return df_filtered

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π —Ç–æ–Ω–Ω—ã: {e}")
        return None


def find_columns(columns):
    """
    –ù–∞—Ö–æ–¥–∏—Ç –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é –Ω–∞–∑–≤–∞–Ω–∏–π
    """
    column_mapping = {}

    for target_name, keywords in COLUMN_PATTERNS.items():
        for col in columns:
            col_lower = str(col).lower()
            if all(keyword in col_lower for keyword in keywords):
                column_mapping[target_name] = col
                break
    return column_mapping


async def find_url_for_date(session: aiohttp.ClientSession, target_date: str):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∏—â–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –¥–∞—Ç—É
    """
    try:
        date_obj = datetime.strptime(target_date, DATE_FORMAT)
        date_formatted = date_obj.strftime(DATE_FORMAT_SPIMEX)

        file_url, file_response, last_date, first_date = await search_in_page(
            session, BASE_URL, date_formatted, page_num=1
        )
        if file_url:
            return file_url, file_response

        page_range = list(range(MAX_PAGES_TO_CHECK, 1, -1))

        for page_num in page_range:
            page_url = BASE_URL + PAGE_URL_PATTERN.format(page_num=page_num)
            file_url, file_response, last_date, first_date = (
                await search_in_page(
                    session, page_url, date_formatted, page_num
                )
            )

            if last_date and last_date > date_formatted:
                break

            if first_date and first_date < date_formatted:
                continue

            if file_url:
                return file_url, file_response

        return None, None

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ HTML: {e}")
        return None, None


async def search_in_page(
    session: aiohttp.ClientSession, page_url: str, date_formatted: str, page_num: int
):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∏—â–µ—Ç —Ñ–∞–π–ª –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    """
    global PAGE_CACHE

    if page_num in PAGE_CACHE:
        cached_data = PAGE_CACHE[page_num]
        first_file_date = cached_data["first_date"]
        last_file_date = cached_data["last_date"]

        if first_file_date and first_file_date < date_formatted:
            return None, None, last_file_date, first_file_date
    else:
        try:
            async with session.get(
                page_url, timeout=aiohttp.ClientTimeout(total=HTTP_TIMEOUT)
            ) as response:
                if response.status != 200:
                    return None, None, None, None

                html_content = await response.text()

                all_files_pattern = ALL_FILES_PATTERN
                all_matches = re.findall(all_files_pattern, html_content)

                first_file_date = None
                last_file_date = None
                if all_matches:
                    first_file_date = all_matches[0][1]
                    last_file_date = all_matches[-1][1]

                    PAGE_CACHE[page_num] = {
                        "first_date": first_file_date,
                        "last_date": last_file_date,
                    }
        except Exception:
            return None, None, None, None

        if first_file_date and first_file_date < date_formatted:
            return None, None, last_file_date, first_file_date

    try:
        async with session.get(
            page_url, timeout=aiohttp.ClientTimeout(total=HTTP_TIMEOUT)
        ) as response:
            if response.status != 200:
                return (
                    None,
                    None,
                    first_file_date if page_num in PAGE_CACHE else None,
                    last_file_date if page_num in PAGE_CACHE else None,
                )
            html_content = await response.text()
    except Exception:
        return (
            None,
            None,
            first_file_date if page_num in PAGE_CACHE else None,
            last_file_date if page_num in PAGE_CACHE else None,
        )

    pattern = FILE_URL_PATTERN.format(date_formatted=date_formatted)
    target_matches = re.findall(pattern, html_content)

    if target_matches:
        file_path = target_matches[0]

        if file_path.startswith("/"):
            file_url = f"https://spimex.com{file_path}"
        else:
            file_url = f"https://spimex.com/{file_path}"

        try:
            async with session.get(
                file_url, timeout=aiohttp.ClientTimeout(total=HTTP_TIMEOUT)
            ) as file_response:
                if file_response.status == 200:
                    file_content = await file_response.read()
                    return (
                        file_url,
                        file_content,
                        last_file_date if page_num in PAGE_CACHE else None,
                        first_file_date if page_num in PAGE_CACHE else None,
                    )
        except Exception:
            pass

    return (
        None,
        None,
        last_file_date if page_num in PAGE_CACHE else None,
        first_file_date if page_num in PAGE_CACHE else None,
    )


async def parse_bulletin_for_date(session: aiohttp.ClientSession, date_str: str):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–∞—Ä—Å–∏—Ç –±—é–ª–ª–µ—Ç–µ–Ω—å –ø–æ –∏—Ç–æ–≥–∞–º —Ç–æ—Ä–≥–æ–≤ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∞—Ç—ã
    """
    print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç—ã: {date_str}")

    url, response_content = await find_url_for_date(session, date_str)

    if not url:
        print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω URL –¥–ª—è –¥–∞—Ç—ã {date_str}")
        return None

    print(f"üì• –ù–∞–π–¥–µ–Ω URL: {url}")

    df = None
    engine = EXCEL_ENGINE

    try:
        df = pd.read_excel(BytesIO(response_content), engine=engine)
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω–æ —Å –¥–≤–∏–∂–∫–æ–º {engine}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å –¥–≤–∏–∂–∫–æ–º {engine}: {e}")
        return None

    if df is None:
        print(f"üìÑ –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å Excel —Ñ–∞–π–ª –Ω–∞ {date_str}")
        return None

    df_metric_ton = extract_metric_ton_data(response_content, date_str)

    if df_metric_ton is None or df_metric_ton.empty:
        print(f"‚ÑπÔ∏è –î–∞–Ω–Ω—ã—Ö –ø–æ –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π —Ç–æ–Ω–Ω–µ –Ω–µ—Ç –Ω–∞ {date_str}")
        return None

    df_metric_ton = df_metric_ton[df_metric_ton["count"] > 0]

    if df_metric_ton.empty:
        print(f"‚ÑπÔ∏è –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–æ–≥–æ–≤–æ—Ä–æ–≤ > 0 –Ω–∞ {date_str}")
        return None

    df = df_metric_ton

    df["oil_id"] = df["exchange_product_id"].str[:4]
    df["delivery_basis_id"] = df["exchange_product_id"].str[4:7]
    df["delivery_type_id"] = df["exchange_product_id"].str[-1]

    df["date"] = pd.to_datetime(date_str)
    now = pd.Timestamp.now()
    df["created_on"] = now
    df["updated_on"] = now

    records = df[RECORDS_TO_SAVE].to_dict(orient="records")

    async with AsyncSessionLocal() as session_db:
        try:
            from sqlalchemy import text

            existing_count_result = await session_db.execute(
                text(
                    "SELECT COUNT(*) FROM spimex_trading_results WHERE DATE(date) = :date_val"
                ),
                {"date_val": pd.to_datetime(date_str).date()},
            )
            existing_count = existing_count_result.scalar()

            if existing_count > 0:
                print(
                    f"‚ÑπÔ∏è –î–∞–Ω–Ω—ã–µ –∑–∞ {date_str} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤ –ë–î "
                    f"({existing_count} –∑–∞–ø–∏—Å–µ–π), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º"
                )
                return None

            for record in records:
                trading_result = TradingResult(**record)
                session_db.add(trading_result)

            await session_db.commit()
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(records)}")
            return records

        except Exception as e:
            await session_db.rollback()
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –ë–î: {e}")
            return None


async def parse_multiple_dates(date_strings: list, max_concurrent: int = 50):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–∞—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    """
    connector = aiohttp.TCPConnector(limit=100, limit_per_host=20)
    timeout = aiohttp.ClientTimeout(total=HTTP_TIMEOUT)

    async with aiohttp.ClientSession(
        connector=connector, timeout=timeout
    ) as session:
        semaphore = asyncio.Semaphore(max_concurrent)

        async def parse_with_semaphore(date_str):
            async with semaphore:
                return await parse_bulletin_for_date(session, date_str)

        tasks = [parse_with_semaphore(date_str) for date_str in date_strings]
        results = await tqdm.gather(*tasks, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç", unit="–¥–∞—Ç–∞")

        return results
